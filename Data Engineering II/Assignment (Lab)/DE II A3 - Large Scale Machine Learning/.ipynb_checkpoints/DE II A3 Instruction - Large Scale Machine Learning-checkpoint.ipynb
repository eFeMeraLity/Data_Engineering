{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DE II A3 Instruction - Large Scale Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Distributed machine learning\n",
    "\n",
    "This part constitutes the mandatory part of the assignment, and its successful completion awards 1 point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Hyperparameter tuning\n",
    "\n",
    "A common task in all machine learning pipelines is to tune so-called hyperparameters of the ML algorithms. These are parameters involved in the training process that are not learned from the data itself, but typically related to the optimization routine used. Examples include the learning rate for SGD, penalty terms, batch sizes, and many other things. A good introduction to the problem with examples for sklearn using cross-validation is found here: [3.2. Tuning the hyper-parameters of an estimator](https://scikit-learn.org/stable/modules/grid_search.html)\\\n",
    "所有机器学习管道中的一个共同任务是调整ML算法的所谓超参数。这些是训练过程中涉及的参数，不是从数据本身学到的，但通常与所使用的优化例程有关。示例包括SGD的学习速率、惩罚条款、批大小以及许多其他东西。通过使用交叉验证的sklearn示例可以很好地介绍这个问题：[3.2. Tuning the hyper-parameters of an estimator](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "\n",
    "The problem is that since the ML model training is often expensive, hyperparameter tuning becomes a highly computationally demanding and hence time-consuming process. Fortunately, it is possible to treat it as a task-parallel problem, where the ML-model training is systematically repeated for different combinations of hyperparameters, in the search for optimally performing combinations in the global space of hyperparameters. Strategies range from simple ones like exhaustive Random search and Grid Search, to more sophisticated alternatives like Bayesian Optimization. There are several established frameworks for distributed parameter tuning such as [RayTune](https://docs.ray.io/en/master/tune/index.html), [HyperOpt](https://github.com/hyperopt/hyperopt) and [DaskML](https://ml.dask.org/hyper-parameter-search.html). Spark, which you encountered in Data Engineering I, also provides [functionality for models made in Spark MLLib](https://spark.apache.org/docs/latest/ml-tuning.html).\\\n",
    "问题是，由于ML模型训练通常是昂贵的，超参数调优成为一个高计算要求和因此耗时的过程。幸运的是，可以把它看作是一个任务并行问题，其中ml模型训练系统地重复对不同的超参数组合，在超参数的全局空间中寻找最优的表现组合。策略从简单的像彻底的随机搜索和网格搜索，到更复杂的备选方案像贝叶斯优化。有几个已建立的用于分布式参数调优的框架，如[Ray | Tune: Scalable Hyperparameter Tuning](https://docs.ray.io/en/master/tune/index.html)、[HyperOpt](https://github.com/hyperopt/hyperopt)和[DaskML](https://ml.dask.org/hyper-parameter-search.html)。您在数据工程I中遇到的Spark也提供了[用于在Spark MLLib中制作的模型的功能](https://spark.apache.org/docs/latest/ml-tuning.html)。\n",
    "\n",
    "Your task now is to use scikit-learn to train a RandomForestClassifier [sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for predicting the forest cover type. The dataset can be downloaded directly via the sklearn API: [sklearn.datasets.fetch_covtype](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype) and there are several notebooks on Kaggle where you can learn about the data and background to the dataset: [Forest Cover Type Prediction](https://www.kaggle.com/c/forest-cover-type-prediction)\\\n",
    "您现在的任务是使用scikit-learn来训练RandomForestClassifier [sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)来预测森林覆盖类型。数据集可以直接通过sklearn API下载：[sklearn.datasets.fetch_covtype](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype)，以及Kaggle上有几个笔记本可以了解数据集和背景：[森林覆盖类型预测](https://www.kaggle.com/c/forest-cover-type-prediction)\n",
    "\n",
    "Do the following:\n",
    "1. Set up the problem using sklearn. Explore the parameters used in the Random Forest implementation. Hint: use the `get_param()` method. Train the model using the default parameters.\\\n",
    "使用sklearn设置问题。探索随机森林实现中使用的参数。提示:使用`get_param()`方法。使用默认参数训练模型。\n",
    "2. Implement a distributed hyperparameter tuning pipeline for tuning the parameters `max_depth`, `n_estimators` and `ccp_alpha` using Ray Tune and an [exhaustive Grid Search strategy](https://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search). Configure, deploy and run it on up to 3 VMs of \"small\" flavor.\\\n",
    "使用Ray Tune和详尽的网格搜索策略实现一个分布式超参数调优管道，用于调优参数`max_depth`，`n_estimators`和`ccp_alpha`。在最多3个“小型”虚拟机上配置、部署和运行它。\n",
    "\n",
    "Please report on:\n",
    "1. The hyperparameters found and the associated cross-validation score. How does it compare to the score for the default parameters?\\\n",
    "找到的超参数和相关的交叉验证评分。它与默认参数的得分如何比较?\n",
    "2. The time taken to complete the tuning, when using 1, 2 and 3 VMs of \"small\" flavor.\\\n",
    "当使用1、2和3个“小型”VMs时，完成调优所花费的时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Distributed training of deep neural networks\n",
    "\n",
    "Another challenge that calls for large scale computing is when training very large models, and/or when the training dataset is very large. Again, distributed computing provides a solution to speed up the training process, but now it is less straight-forward how to achieve parallelism.\\\n",
    "另一个需要大规模计算的挑战是当训练非常大的模型和/或训练数据集非常大时。同样，分布式计算提供了一种解决方案来加快训练过程，但是现在如何实现并行性没有那么直接。\n",
    "\n",
    "In this task, which will be mostly theoretical in nature (unfortunately we are not able to provide access to GPU clusters), you will explore available approaches to scale training by leveraging multiple GPUs/TPUs.\\\n",
    "在本任务中，本质上主要是理论性的(不幸的是，我们不能提供对GPU集群的访问)，您将探索通过利用多个GPU/TPU来扩展培训的可用方法。\n",
    "\n",
    "Since you are familiar with the PyTorch library, start by reading up on this background:\\\n",
    "既然您熟悉PyTorch库，那么请先阅读下面的背景知识:\n",
    "* Background on data parallelism: https://en.wikipedia.org/wiki/Data_parallelism\n",
    "* Implementation of DataParallel training in PyTorch: http://www.vldb.org/pvldb/vol13/p3005-li.pdf\n",
    "* Documentation for PyTorch Distributed: https://pytorch.org/tutorials/beginner/dist_overview.html\n",
    "\n",
    "Then study and do a tutorial using another broadly used ML stack - Keras/Tensorflow. While the APIs and implementations are different, you will recognize the common themes for parallelism:\\\n",
    "然后学习和做一个教程使用另一个广泛使用的ML堆栈- Keras/Tensorflow。虽然API和实现是不同的，但你会发现并行性的共同主题:\n",
    "1. Distributed Tensorflow (on CPU)\n",
    "    1. [使用 TensorFlow 进行分布式训练](https://www.tensorflow.org/guide/distributed_training)\n",
    "        1. Do this tutorial in Google Colab: [Distributed training with TensorFlow](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/distributed_training.ipynb)\n",
    "\n",
    "Equipped with this background knowledge, explain (in max 0.5 page 11pt Arial) the main difference between data parallel training strategies and model parallel training strategies. In which situations are the respective strategies most appropriate to use? Assuming you also need to do hyperparameter tuning, in which situations would you consider distributing the training of individual neural networks rather than distributing test cases for the tuning pipeline?\\\n",
    "有了这些背景知识，解释(最多0.5页11pt Arial)数据并行训练策略和模型并行训练策略之间的主要区别。在哪些情况下，各自的策略最适合使用？假设您还需要进行超参数调优，在哪种情况下您会考虑分配单个神经网络的训练，而不是为调优管道分配测试用例？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Federated Learning\n",
    "\n",
    "Awards up to 2 extra credits:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated machine learning is an emerging class of distributed and/or decentralized schemes that sets out to solve privacy problems in machine learning. In many situations, multiple organizations, each having private datasets, might want to collaborate to jointly train a ML model, without disclosing their private data to other parties.\\\n",
    "联邦机器学习是一类新兴的分布式和/或去中心化方案，旨在解决机器学习中的隐私问题。在许多情况下，多个组织(每个组织都有私有数据集)可能希望协作共同训练ML模型，而不向其他方披露它们的私有数据。\n",
    "\n",
    "1. Read this introduction to Federated Learning:\n",
    "https://en.wikipedia.org/wiki/Federated_learning\n",
    "2. Read this paper about Federated Averaging: https://arxiv.org/pdf/1602.05629.pdf\n",
    "3. Read our preprint about FEDn: https://arxiv.org/abs/2103.00148\n",
    "\n",
    "[FEDn](https://github.com/scaleoutsystems/fedn) is a software developed jointly by the ISCL lab and the spin-off company Scaleout Systems. It can be used to set up production systems for horizontal federated learning in a ML-framework agnostic manner.\\\n",
    "[FEDn](https://github.com/scaleoutsystems/fedn)是由ISCL实验室和分拆公司Scaleout Systems联合开发的软件。它可用于以一种ml框架不可知的方式设置用于水平联邦学习的生产系统。\n",
    "\n",
    "**Task:** Use FEDn to set up and run fully distributed federated training for either the Keras or the PyTorch MNIST example (bundled in the repository) on three virtual machines. Use one for the base services (MongoDB and Minio)+reducer, one for one combiner and one to run 2 clients. Include a snapshot of the training result in the Dashboard. Then answer the following questions:\\\n",
    "使用FEDn在三个虚拟机上为Keras或PyTorch MNIST示例(绑定在存储库中)设置和运行完全分布式的联邦训练。使用一个用于基础服务(MongoDB和Minio)+reducer，一个用于一个combiner，一个用于运行2个客户端。在仪表板中包含培训结果的快照。然后回答以下问题:\n",
    "\n",
    "1. Write a short (max 0.5 page Arial 11pt) reflection on how federated learning differs from distributed machine learning. Focus on statistical and system heterogeneity.\\\n",
    "就联邦学习与分布式机器学习的不同之处写一个简短的反思(最多0.5页Arial 11pt)。关注统计和系统异构性。\n",
    "2. Write a short (max 0.5 page Arial 11pt) discussion on how federated learning relates to the Parameter Server strategy for distributed machine learning.\\\n",
    "写一个简短的(最多0.5页Arial 11pt)讨论联邦学习如何与参数服务器策略的分布式机器学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
