{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions for the written part of the literature seminar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Contextualization and orchestration\n",
    "Q1: What is the meaning and importance of contextualization and orchestration, and what are currently available tools and services in this area?\\\n",
    "contextualization和orchestration的意义和重要性是什么？该领域目前有哪些可用的工具和服务？\n",
    "\n",
    "### Contextualization\n",
    "In cloud computing, contextualization means providing customized computing environment or allowing a virtual machine instance to learn about its cloud environment and user requirement (the 'context') and configure itself to run correctly.\\\n",
    "在云计算中，上下文化意味着提供定制的计算环境，或允许虚拟机实例了解其云环境和用户需求(“上下文”)，并配置自身以正确运行。\n",
    "* Lecture\n",
    "\n",
    "Each VM instance started from a template needs to be customized with some unique settings. The settings need to be applied dynamically, normally done as part of the VM boot process. This boot-time customization process is called contextualization.\\\n",
    "每个从模板启动的虚拟机实例都需要自定义一些独特的设置。需要动态应用这些设置，通常是在VM引导过程中进行的。这个引导时定制过程称为上下文化。\n",
    "\n",
    "Contextualization is the autonomous configuration of individual components of an application and supporting software stack during deployment to a specific environment.\\\n",
    "上下文化是指在部署到特定环境期间对应用程序的各个组件和支持软件堆栈的自主配置。\n",
    "* [Contextualization: dynamic configuration of virtual machines](https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-015-0042-8)\n",
    "\n",
    "Contextualization is then the process of identifying the data relevant to an entity based on the entity's contextual information. Contextualization excludes irrelevant data from consideration and has the potential to reduce data from several aspects including volume, velocity, and variety in large-scale data intensive applications (Yavari et al.).\\\n",
    "上下文化是基于实体的上下文信息识别与实体相关的数据的过程。上下文化将不相关的数据排除在考虑范围之外，并有可能从多个方面减少数据，包括大规模数据密集型应用程序的容量、速度和多样性(Yavari等)。\n",
    "* [Contextualization (computer science)](https://en.wikipedia.org/wiki/Contextualization_(computer_science))\n",
    "\n",
    "**Importance**\n",
    "\n",
    "* Provide scalable solution\n",
    "* No need to manage fat images\n",
    "* Dynamic configuration\n",
    "\n",
    "Modern virtualization technologies enable rapid provisioning of Virtual Machines (VMs) and thus allow cloud services to scale up and down on-demand. This elasticity comes with a new set of challenges for dynamic service configuration. Contextualization is a set of processes and mechanisms that enable a service to scale elastically alongside the resources and software that support it through the orchestration of these dependencies toward the common goals of the service.\\\n",
    "现代虚拟化技术支持快速提供虚拟机(vm)，从而允许云服务按需伸缩。这种弹性为动态服务配置带来了一组新的挑战。上下文化是一组流程和机制，通过编排这些依赖关系，使服务能够与支持它的资源和软件一起灵活地伸缩，以达到服务的共同目标。\n",
    "* [Towards a Contextualization Solution for Cloud Platform Services](https://ieeexplore.ieee.org/document/6133160)\n",
    "\n",
    "Each VM instance started from a template needs to be customized with some unique settings, e.g., networking configuration to ensure each instance is assigned a unique IP-address. The settings need to be applied dynamically, normally done as part of the VM boot process. This boot-time customization process is called **contextualization** [2, 3].\\\n",
    "从模板启动的每个虚拟机实例都需要定制一些独特的设置，例如，网络配置，以确保每个实例都被分配一个唯一的ip地址。需要动态应用这些设置，通常是在VM引导过程中进行的。这种启动时定制过程称为**上下文化**[2,3]。\n",
    "\n",
    "The contextualization stage is needed to support interactions with any infrastructure-specific services or settings.\\\n",
    "需要上下文化阶段来支持与任何特定于基础设施的服务或设置的交互。\n",
    "* [Contextualization: dynamic configuration of virtual machines](https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-015-0042-8)\n",
    "\n",
    "The main usage of \"contextualization\" is in improving the process of data:\\\n",
    "“上下文化”的主要用途是改善数据处理：\n",
    "\n",
    "Reduce the amount of data: Contextualization has the potential to reduce the amount of data based on the interests from applications/services/users. Contextualization can improve the scalability and efficiency of data process, query, delivery by excluding irrelevant data.\\\n",
    "减少数据量:基于应用/服务/用户的兴趣，上下文化具有减少数据量的潜力。通过排除无关数据，上下文化可以提高数据处理、查询和交付的可伸缩性和效率。\n",
    "\n",
    "As an example, ConTaaS facilitates contextualisation of the data for IoT applications and could improve the processing for large-scale IoT applications from various Big Data aspects including volume, velocity, and variety.\\\n",
    "例如，ConTaaS促进了物联网应用中的数据上下文化，并可以从大数据的各个方面（包括体积、速度和多样性）改进大规模物联网应用的处理。\n",
    "\n",
    "* [Contextualization (computer science)](https://en.wikipedia.org/wiki/Contextualization_(computer_science))\n",
    "\n",
    "### Orchestration\n",
    "Orchestration is a process of resource contextualization based on the automation available in the cloud systems.\\\n",
    "编排是一个基于云系统中可用自动化的资源上下文化过程。\n",
    "\n",
    "Orchestration is the automated configuration, management, and coordination of computer systems, applications, and services. Orchestration helps IT to more easily manage complex tasks and workflows.\\\n",
    "编排是计算机系统、应用程序和服务的自动化配置、管理和协调。编排帮助IT更容易地管理复杂的任务和工作流。\n",
    "* [RedHat | What is orchestration?](https://www.redhat.com/en/topics/automation/what-is-orchestration)\n",
    "\n",
    "**Importance**\n",
    "* A process required for\n",
    "    * rapid application deployment\n",
    "    * scalability\n",
    "    * management\n",
    "    * high availability\n",
    "    * agility\n",
    "* Essential for large complex applications\n",
    "* A process at the level of Platform as a Service (PaaS)\n",
    "\n",
    "### Available tools and services\n",
    "* [Docker Compose](https://www.docker.com/)\\\n",
    "Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration.\\\n",
    "Compose是一个用于定义和运行多容器Docker应用程序的工具。使用Compose，您可以使用YAML文件来配置应用程序的服务。然后，使用一个命令，从您的配置创建并启动所有服务。\n",
    "    * [Overview of Docker Compose](https://docs.docker.com/compose/)\n",
    "\n",
    "* [Kubernetes](http://kubernetes.io/)\\\n",
    "Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\\\n",
    "Kubernetes是一个可移植的、可扩展的、开源的平台，用于管理容器化的工作负载和服务，它促进了声明式配置和自动化。它有一个庞大的、快速增长的生态系统。Kubernetes的服务、支持和工具广泛可用。\n",
    "    * [What is Kubernetes?](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/)\n",
    "\n",
    "* [Ansible](https://www.ansible.com/)\n",
    "Ansible is a radically simple IT automation engine that automates cloud provisioning, configuration management, application deployment, intra-service orchestration, and many other IT needs.\\\n",
    "Ansible是一个非常简单的IT自动化引擎，它可以自动化云供应、配置管理、应用程序部署、服务内部编排和许多其他IT需求。\n",
    "    * [How Ansible Works](https://www.ansible.com/overview/how-ansible-works)\n",
    "\n",
    "* [Heat](https://wiki.openstack.org/wiki/Heat)\n",
    "Heat is the main project in the OpenStack Orchestration program. It implements an orchestration engine to launch multiple composite cloud applications based on templates in the form of text files that can be treated like code. A native Heat template format is evolving, but Heat also endeavours to provide compatibility with the AWS CloudFormation template format, so that many existing CloudFormation templates can be launched on OpenStack. Heat provides both an OpenStack-native ReST API and a CloudFormation-compatible Query API.\\\n",
    "Heat是OpenStack业务流程中的主要项目。它实现了一个编排引擎，以基于可以像代码一样处理的文本文件形式的模板来启动多个复合云应用程序。一个本地的Heat模板格式正在演变，但Heat也努力提供与AWS CloudFormation模板格式的兼容性，以便许多现有的CloudFormation模板可以在OpenStack上发布。Heat提供了一个openstack本地ReST API和一个兼容cloudform的查询API。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Serverless architecture\n",
    "Q2: How does contemporary serverless architecture relate to the digitalization of society and Big Data applications?\\\n",
    "当代无服务器架构如何与社会数字化和大数据应用相关？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Logical progression in a seemingly fragmented ecosystem\n",
    "Q3: Try to, from a technological point of view, relate the following Products/cloud providers, frameworks and tools to each other, both historically and in the technological problems they have addressed. Is there some logical progression in a seemingly fragmented ecosystem?\\\n",
    "尝试从技术的角度，将以下产品/云提供商、框架和工具相互联系起来，既有历史意义，也有它们所解决的技术问题。在一个看似支离破碎的生态系统中是否存在某种逻辑进程?\n",
    "* Set-A: Infrastructure and service related tools and technologies: OpenStack, Google App Engine, Kubernetes, Ansible, Lamda functions, OpenFaaS\n",
    "* Set-B: Stream processing and model training related frameworks: Apache Kafka, Apache Pulsar, Apache Spark, TensorFlow, MLFlow, Stackn (of course, include more tools and frameworks if you find things that interests you)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Model serving and CI/CD\n",
    "Q4: Explain the role and importance of model serving and CI/CD in today’s data engineering world. Write down the names and briefly explain four frameworks (two for model serving and two for CI/CD) that offer model serving and CI/CD capabilities.\\\n",
    "解释模型服务和CI/CD在当今数据工程世界中的作用和重要性。写下名称并简要解释四个提供模型服务和CI/CD功能的框架(两个用于模型服务，两个用于CI/CD)。\n",
    "\n",
    "### Model serving\n",
    "Model serving defines the frameworks to serve the models that have been trained using machine learning in to production. It allows data scientists to deploy a growing number of models independently with minimal data engineering and DevOps effort in each deployment. When the ML model development process is finished, the developed models are to be deployed and composed as an application. The heterogeneity of infrastructures makes the deployment very complicated and difficult to automate. Hence, for deployment of trained model there should be considerations which should be in line of:\\\n",
    "模型服务定义了为生产中使用机器学习训练过的模型服务的框架。它允许数据科学家独立部署越来越多的模型，每次部署都只需要最少的数据工程和DevOps工作。当ML模型开发流程完成时，将部署所开发的模型并将其组合为一个应用程序。基础设施的异构性使得部署非常复杂，难以实现自动化。因此，在部署训练有素的模式时，应考虑以下因素：\n",
    "* How to wrap the prediction code as a production-ready service?\n",
    "* How to ship and load the model file?\n",
    "* Scalability, Throughput, Latency.\n",
    "* Deployments — How to deploy new model versions? How to rollback? \n",
    "* How to make the deployment process easy and accessible for Data Scientist?\n",
    "\n",
    "There are several model serving frameworks available which provide solutions for above considerations. We will discuss about few frameworks here.\\\n",
    "有几个可用的模型服务框架为上述考虑提供了解决方案。我们将在这里讨论几个框架。\n",
    "\n",
    "**TensorFlow Serving:**\n",
    "\n",
    "This is a flexible, high-performance serving system designed for production environments for ML models.It can server both Tensorflow model and models from other frameworks. It takes models after training and manages their lifetimes to provide with versioned access via a high-performance, reference-counted lookup table. Following are some important features of TensorFlow serving framework.\\\n",
    "这是一个为ML模型的生产环境设计的灵活、高性能的服务系统。它可以服务于Tensorflow模型和来自其他框架的模型。它采用经过培训的模型，并管理它们的生命周期，通过一个高性能的引用计数查找表提供版本化的访问。以下是TensorFlow服务框架的一些重要特性。\n",
    "* Can serve multiple models, or multiple versions of the same model at the same time\n",
    "* Allows deployment of new model versions without changing your code\n",
    "* Its efficient, low-overhead implementation adds minimal latency to inference time \n",
    "* Supports many servables: Tensorflow models,vocabularies, feature transformations, and non-Tensorflow-based machine learning model.\n",
    "\n",
    "**Clipper:**\n",
    "\n",
    "Clipper is a model serving implementation of model as a code approach. Clipper is a low-latency prediction serving system which supports serving of Caffe, TensorFlow, and Scikit-learn models. It has a layered architecture system which reduces the complexity of implementing a prediction serving stack and achieves three crucial properties of a prediction serving system: low latencies, high throughputs, and improved accuracy.\\\n",
    "Clipper是一种以代码方式实现模型的模型。Clipper是一个低延迟的预测服务系统，支持Caffe、TensorFlow和Scikit-learn模型的服务。它采用分层体系结构，降低了实现预测服务栈的复杂性，并实现了预测服务系统的三个关键属性:低延迟、高吞吐量和提高精度。\n",
    "\n",
    "Clipper is divided into two layers: the model abstraction layer, and the model selection layer. The first layer exposes a common API that abstracts away the heterogeneity of existing ML frameworks and models. The model selection layer sits above the model abstraction layer and dynamically selects and combines predictions across competing models to provide more accurate and robust predictions.Here are some important features of clipper.\\\n",
    "Clipper分为两层:模型抽象层和模型选择层。第一层公开了一个公共API，它抽象了现有ML框架和模型的异构性。模型选择层位于模型抽象层之上，并动态地选择和组合跨竞争模型的预测，以提供更准确和健壮的预测。这里有一些重要的特点，clipper。\n",
    "* Enables us to do deploy your model by running just a few lines of code and with support for a lot of machine learning frameworks.\n",
    "* Creates Docker containers out of your models for simple cluster and resource management.\n",
    "* Makes it easy to update or rollback models.\n",
    "\n",
    "### CI/CD\n",
    "This is a set of operating principles which enables more frequent and reliable delivery of code changes to production.The technical goal of CI is to establish a consistent and automated way to build, package, and test applications. Continuous delivery starts up where continuous integration ends. CD automates the delivery of applications to selected infrastructure environments\\cite{ContinuousDelivery}. CI/CD tools help store the environment-specific parameters that must be packaged with each delivery. CI/CD automation then performs any necessary service calls to web servers, databases, and other services that may need to be restarted or follow other procedures when applications are deployed.\\\n",
    "这是一组操作原则，可以更频繁、更可靠地向产品交付代码更改。CI的技术目标是建立一种一致的自动化方法来构建、打包和测试应用程序。持续交付开始于持续集成结束之处。CD自动将应用程序交付到选定的基础设施环境(引用{ContinuousDelivery})。CI/CD工具帮助存储必须与每次交付一起打包的特定于环境的参数。然后CI/CD自动化对web服务器、数据库和其他服务执行任何必要的服务调用，这些服务在部署应用程序时可能需要重新启动或遵循其他过程。\n",
    "\n",
    "**Jenkins**\n",
    "\n",
    "Jenkins is an open-source automation server ,with many plugins ,in which the central build and continuous integration process take place.  It is used to build and test the software projects continuously making it easier for developers to integrate changes to the project, and making it easier for users to obtain a fresh build. It also allows us to continuously deliver our software by integrating with a large number of testing and deployment technologies.\\\n",
    "Jenkins是一个开源的自动化服务器，有许多插件，在其中进行中心构建和持续集成过程。它被用来不断地构建和测试软件项目，使开发人员更容易地集成对项目的更改，并使用户更容易获得一个新的构建。它还允许我们通过集成大量的测试和部署技术来持续交付我们的软件。\n",
    "\n",
    "**GitLab**\n",
    "\n",
    "GitLab is a web-based Git repository that provides free open and private repositories. It is a complete DevOps platform that enables us to perform all the tasks in a project—from project planning and source code management to monitoring and security. It allows us to build powerful CI/CD pipelines. It allows us to trigger builds, run tests, and deploy code with each commit or push.\\\n",
    "GitLab是一个基于web的Git存储库，提供免费的开放和私有存储库。它是一个完整的DevOps平台，使我们能够执行项目中的所有任务——从项目规划和源代码管理到监控和安全。它允许我们建立强大的CI/CD管道。它允许我们在每次提交或推送时触发构建、运行测试和部署代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Distributed and Federated machine learning\n",
    "Q5: What is the difference between distributed and Federated machine learning? Highlight the architectural differences between the two machine learning environments. Which one of them is more suitable for privacy preserving model training process and why?\\\n",
    "分布式机器学习和联邦机器学习有什么区别？强调两种机器学习环境之间的架构差异。其中哪一个更适合于隐私保护模型训练过程，为什么？\n",
    "\n",
    "### Distributed machine learning\n",
    "Usually, when solving a machine learning problem, two things are necessary, data is the first and a model is the second, where the model is improved iteratively by training it on the data. This procedure can be done on a single machine. The model in itself consists of parameters, therefore when the size of the data or the number of model parameters becomes big, training the model can no longer be performed on a single machine, and that's where distributed machine learning comes into use.\\\n",
    "通常，当解决一个机器学习问题时，有两件事是必须的，数据是第一，模型是第二，模型通过对数据进行训练来不断改进。这个过程可以在一台机器上完成。模型本身由参数组成，因此当数据的大小或模型参数的数量变大时，模型的训练就不能再在一台机器上进行了，这就是分布式机器学习的应用。\n",
    "\n",
    "The main idea is that instead of a single machine, a cluster of machines can be setup where each node contributes to the training of the model. Either the data can be partitioned and distributed over the nodes, or the model can be divided and each node trains a part of the model. The combination of partitioning data and model is also possible. Each node trains the model with the subset of data allocated to it, sends the trained model back to a master machine. The models of all nodes are aggregated and a master model is created from them through ensembling, for example by taking a majority vote for a single test data point.\\\n",
    "其主要思想是，可以设置一个机器集群，其中每个节点都有助于模型的训练，而不是单个机器。要么可以对数据进行分区并在节点上分布，要么可以对模型进行划分，每个节点训练模型的一部分。分区数据和模型的组合也是可能的。每个节点用分配给它的数据子集训练模型，然后将训练后的模型发送回主机器。聚合所有节点的模型，并通过集成(例如通过对单个测试数据点进行多数投票)从它们创建一个主模型。\n",
    "\n",
    "According to \\cite{distributedML} there are four topologies for distributed machine learning, namely centralized(ensembling), decentralized(tree), decentralized(parameter server) and fully distributed. The first among these is already described. In a decentralized tree structure, data is partitioned among multiple computing nodes, and each node has a copy of the model, but instead of each node communicating directly with the master server, there is the possibility for intermediate model aggregation, where a couple of nodes can have a parent node, and the model parameters(or gradients) are aggregated and broacasted back and forth, until the optimal model is found. This way, the cluster can easily be scaled up and the communication time only grows logarithmically, instead of linearly. It can also be the case that the nodes train different parts of the model, send those parts to different parameter servers, where those servers recombine the different parts into a new model. This is decentralized parameter sever structure. The fourth and last structure is fully distributed cluster, where there is no master server managing the cluster, instead each computing node can communicate with all other nodes, exchange data and model parameters until all nodes find the optimal model.\\\n",
    "根据distributedML，分布式机器学习有四种拓扑结构，即集中(集成)、分散(树)、分散(参数服务器)和完全分布。其中第一个已经描述过了。在分散的树结构中，数据被划分到多个计算节点中，每个节点都有一个模型的副本，但不是每个节点都直接与主服务器通信，而是有可能进行中间模型聚合，其中两个节点可以有一个父节点，对模型参数(或梯度)进行反复聚合和叠加，直到找到最优模型。这样，集群可以很容易地扩展，通信时间只以对数增长，而不是线性增长。也可能是节点训练模型的不同部分，将这些部分发送到不同的参数服务器，这些服务器将不同的部分重新组合到一个新的模型中。这是分散的参数服务器结构。第四种也是最后一种结构是全分布式集群，没有主服务器管理集群，每个计算节点可以与所有其他节点通信，交换数据和模型参数，直到所有节点找到最优模型。\n",
    "\n",
    "### Federated machine learning\n",
    "There are several issues with these structures, both privacy and security issues. Firstly, it may be the case that data cannot be collected in advance. Even more worrying is the exchange of data between computing nodes, which can lead to privacy breaches. Regarding security issues, each computing node can be prone to adversarial attacks, or the data manager(in case one exist) can be attacked and data is leaked, or the parameter server can be attacked and the model become poisoned.\\\n",
    "这些结构存在几个问题，包括隐私和安全问题。首先，可能会出现数据无法提前收集的情况。更令人担忧的是计算节点之间的数据交换，这可能导致隐私泄露。在安全问题上，每个计算节点都容易受到对抗性攻击，或者数据管理器(如果存在数据管理器)受到攻击而数据泄露，或者参数服务器受到攻击而模型中毒。\n",
    "\n",
    "To solve some of these issues, federated machine learning were introduced. As the name suggests, federated learning means a group of independent computing nodes work together to form a centralized cluster. The main difference between distributed and federated learning is that in federated learning data is not collected in advance and partitioned, but instead each node has access to its own data. The nodes are typically smart sensors, smartphones and tablets that can on their own generate data. The types of models trained in this fashion are those that a node(user in this case) can directly benefit from, for example voice recognition and text auto-completion, as proposed by \\cite{federatedNN}. There is a master server that selects a task and a model, sends a copy of the model to each node, where the model is trained simultaneously by each node on different data. The trained models are encrypted and sent to a virtual aggregator for compiling the models, and sends them back to the master server where the main model is updated and broadcasted back to the computing nodes. This way, no data is ever exchanged between nodes and there is no communication between nodes. Thus there is a higher level of privacy preserving. Also, the communication between the nodes and master server is less than distributed learning, enabling higher security. However, according to \\cite{federatedML}, even federated learning is not fully secure, as the data and model in each computing node can still be prone to attacks, as well as the virtual aggregator and master server.\\\n",
    "为了解决这些问题，引入了联邦机器学习。顾名思义，联合学习是指一组独立的计算节点一起工作，形成一个集中的集群。分布式学习和联邦学习的主要区别在于，在联邦学习中，数据不是预先收集和分区的，而是每个节点都可以访问自己的数据。这些节点通常是智能传感器、智能手机和平板电脑，它们自己可以生成数据。以这种方式训练的模型类型是节点(本例中是用户)可以直接受益的模型类型，如\\cite{federatedNN}提出的语音识别和文本自动补全。有一个主服务器，它选择一个任务和一个模型，向每个节点发送模型的副本，每个节点在不同的数据上同时对模型进行训练。经过训练的模型被加密并发送到一个虚拟聚合器用于编译模型，然后将它们发送回主服务器，在主服务器上更新主模型并将其广播回计算节点。这样，节点之间就不会交换数据，节点之间也不会进行通信。这样就有了更高层次的隐私保护。同时，节点与主服务器之间的通信少于分布式学习，安全性更高。但是，据federatedML说，即使是联邦学习也不是完全安全的，因为每个计算节点中的数据和模型，以及虚拟聚合器和主服务器仍然容易受到攻击。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
